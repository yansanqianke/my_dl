{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 利用GPU训练(方式一)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "① GPU训练主要有三部分，网络模型、数据(输入、标注)、损失函数，这三部分放到GPU上。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T13:02:00.557732Z",
     "start_time": "2025-02-17T13:01:26.308283Z"
    }
   },
   "source": [
    "import torchvision\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# from model import * 相当于把 model中的所有内容写到这里，这里直接把 model 写在这里\n",
    "class Tudui(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Tudui, self).__init__()        \n",
    "        self.model1 = nn.Sequential(\n",
    "            nn.Conv2d(3,32,5,1,2),  # 输入通道3，输出通道32，卷积核尺寸5×5，步长1，填充2    \n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32,32,5,1,2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32,64,5,1,2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),  # 展平后变成 64*4*4 了\n",
    "            nn.Linear(64*4*4,64),\n",
    "            nn.Linear(64,10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model1(x)\n",
    "        return x\n",
    "\n",
    "# 准备数据集\n",
    "train_data = torchvision.datasets.CIFAR10(\"./data\",train=True,transform=torchvision.transforms.ToTensor(),download=True)\n",
    "test_data = torchvision.datasets.CIFAR10(\"./data\",train=False,transform=torchvision.transforms.ToTensor(),download=True)\n",
    "\n",
    "# length 长度\n",
    "train_data_size = len(train_data)\n",
    "test_data_size = len(test_data)\n",
    "# 如果train_data_size=10，则打印：训练数据集的长度为：10\n",
    "print(\"训练数据集的长度：{}\".format(train_data_size))\n",
    "print(\"测试数据集的长度：{}\".format(test_data_size))\n",
    "\n",
    "# 利用 Dataloader 来加载数据集\n",
    "train_dataloader = DataLoader(train_data, batch_size=64)        \n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "# 创建网络模型\n",
    "tudui = Tudui() \n",
    "if torch.cuda.is_available():\n",
    "    tudui = tudui.cuda() # 网络模型转移到cuda上\n",
    "\n",
    "# 损失函数\n",
    "loss_fn = nn.CrossEntropyLoss() # 交叉熵，fn 是 fuction 的缩写\n",
    "if torch.cuda.is_available():\n",
    "    loss_fn = loss_fn.cuda()        # 损失函数转移到cuda上\n",
    "\n",
    "# 优化器\n",
    "learning = 0.01  # 1e-2 就是 0.01 的意思\n",
    "optimizer = torch.optim.SGD(tudui.parameters(),learning)   # 随机梯度下降优化器  \n",
    "\n",
    "# 设置网络的一些参数\n",
    "# 记录训练的次数\n",
    "total_train_step = 0\n",
    "# 记录测试的次数\n",
    "total_test_step = 0\n",
    "\n",
    "# 训练的轮次\n",
    "epoch = 10\n",
    "\n",
    "# 添加 tensorboard\n",
    "writer = SummaryWriter(\"logs\")\n",
    "\n",
    "for i in range(epoch):\n",
    "    print(\"-----第 {} 轮训练开始-----\".format(i+1))\n",
    "    \n",
    "    # 训练步骤开始\n",
    "    tudui.train() # 当网络中有dropout层、batchnorm层时，这些层能起作用\n",
    "    for data in train_dataloader:\n",
    "        imgs, targets = data\n",
    "        if torch.cuda.is_available():\n",
    "            imgs = imgs.cuda()  # 数据放到cuda上\n",
    "            targets = targets.cuda() # 数据放到cuda上\n",
    "        outputs = tudui(imgs)\n",
    "        loss = loss_fn(outputs, targets) # 计算实际输出与目标输出的差距\n",
    "        \n",
    "        # 优化器对模型调优\n",
    "        optimizer.zero_grad()  # 梯度清零\n",
    "        loss.backward() # 反向传播，计算损失函数的梯度\n",
    "        optimizer.step()   # 根据梯度，对网络的参数进行调优\n",
    "        \n",
    "        total_train_step = total_train_step + 1\n",
    "        if total_train_step % 100 == 0:\n",
    "            print(\"训练次数：{}，Loss：{}\".format(total_train_step,loss.item()))  # 方式二：获得loss值\n",
    "            writer.add_scalar(\"train_loss\",loss.item(),total_train_step)\n",
    "    \n",
    "    # 测试步骤开始（每一轮训练后都查看在测试数据集上的loss情况）\n",
    "    tudui.eval()  # 当网络中有dropout层、batchnorm层时，这些层不能起作用\n",
    "    total_test_loss = 0\n",
    "    total_accuracy = 0\n",
    "    with torch.no_grad():  # 没有梯度了\n",
    "        for data in test_dataloader: # 测试数据集提取数据\n",
    "            imgs, targets = data # 数据放到cuda上\n",
    "            if torch.cuda.is_available():\n",
    "                imgs = imgs.cuda() # 数据放到cuda上\n",
    "                targets = targets.cuda()\n",
    "            outputs = tudui(imgs)\n",
    "            loss = loss_fn(outputs, targets) # 仅data数据在网络模型上的损失\n",
    "            total_test_loss = total_test_loss + loss.item() # 所有loss\n",
    "            accuracy = (outputs.argmax(1) == targets).sum()\n",
    "            total_accuracy = total_accuracy + accuracy\n",
    "            \n",
    "    print(\"整体测试集上的Loss：{}\".format(total_test_loss))\n",
    "    print(\"整体测试集上的正确率：{}\".format(total_accuracy/test_data_size))\n",
    "    writer.add_scalar(\"test_loss\",total_test_loss,total_test_step)\n",
    "    writer.add_scalar(\"test_accuracy\",total_accuracy/test_data_size,total_test_step)  \n",
    "    total_test_step = total_test_step + 1\n",
    "    \n",
    "    torch.save(tudui, \"./model/tudui_{}.pth\".format(i)) # 保存每一轮训练后的结果\n",
    "    #torch.save(tudui.state_dict(),\"tudui_{}.path\".format(i)) # 保存方式二         \n",
    "    print(\"模型已保存\")\n",
    "    \n",
    "writer.close()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据集的长度：50000\n",
      "测试数据集的长度：10000\n",
      "-----第 1 轮训练开始-----\n",
      "训练次数：100，Loss：2.296793222427368\n",
      "训练次数：200，Loss：2.2821974754333496\n",
      "训练次数：300，Loss：2.2831838130950928\n",
      "训练次数：400，Loss：2.209364414215088\n",
      "训练次数：500，Loss：2.117074728012085\n",
      "训练次数：600，Loss：2.0596351623535156\n",
      "训练次数：700，Loss：2.0212600231170654\n",
      "整体测试集上的Loss：316.29375100135803\n",
      "整体测试集上的正确率：0.27070000767707825\n",
      "模型已保存\n",
      "-----第 2 轮训练开始-----\n",
      "训练次数：800，Loss：1.9065762758255005\n",
      "训练次数：900，Loss：1.8732835054397583\n",
      "训练次数：1000，Loss：1.9558640718460083\n",
      "训练次数：1100，Loss：1.9593294858932495\n",
      "训练次数：1200，Loss：1.7373617887496948\n",
      "训练次数：1300，Loss：1.6748982667922974\n",
      "训练次数：1400，Loss：1.7442967891693115\n",
      "训练次数：1500，Loss：1.8172569274902344\n",
      "整体测试集上的Loss：316.2952779531479\n",
      "整体测试集上的正确率：0.2902999818325043\n",
      "模型已保存\n",
      "-----第 3 轮训练开始-----\n",
      "训练次数：1600，Loss：1.7472018003463745\n",
      "训练次数：1700，Loss：1.6404842138290405\n",
      "训练次数：1800，Loss：1.9549872875213623\n",
      "训练次数：1900，Loss：1.7064321041107178\n",
      "训练次数：2000，Loss：1.9717442989349365\n",
      "训练次数：2100，Loss：1.528929352760315\n",
      "训练次数：2200，Loss：1.4722368717193604\n",
      "训练次数：2300，Loss：1.7930965423583984\n",
      "整体测试集上的Loss：267.122754573822\n",
      "整体测试集上的正确率：0.3813000023365021\n",
      "模型已保存\n",
      "-----第 4 轮训练开始-----\n",
      "训练次数：2400，Loss：1.7148257493972778\n",
      "训练次数：2500，Loss：1.3324514627456665\n",
      "训练次数：2600，Loss：1.6071445941925049\n",
      "训练次数：2700，Loss：1.6662517786026\n",
      "训练次数：2800，Loss：1.4968783855438232\n",
      "训练次数：2900，Loss：1.5818531513214111\n",
      "训练次数：3000，Loss：1.3523361682891846\n",
      "训练次数：3100，Loss：1.54717218875885\n",
      "整体测试集上的Loss：255.51994252204895\n",
      "整体测试集上的正确率：0.407399982213974\n",
      "模型已保存\n",
      "-----第 5 轮训练开始-----\n",
      "训练次数：3200，Loss：1.3696218729019165\n",
      "训练次数：3300，Loss：1.4870073795318604\n",
      "训练次数：3400，Loss：1.4639768600463867\n",
      "训练次数：3500，Loss：1.5801441669464111\n",
      "训练次数：3600，Loss：1.5582447052001953\n",
      "训练次数：3700，Loss：1.338739037513733\n",
      "训练次数：3800，Loss：1.2859115600585938\n",
      "训练次数：3900，Loss：1.422303557395935\n",
      "整体测试集上的Loss：249.79589104652405\n",
      "整体测试集上的正确率：0.4235000014305115\n",
      "模型已保存\n",
      "-----第 6 轮训练开始-----\n",
      "训练次数：4000，Loss：1.453203797340393\n",
      "训练次数：4100，Loss：1.4331891536712646\n",
      "训练次数：4200，Loss：1.5306414365768433\n",
      "训练次数：4300，Loss：1.2188220024108887\n",
      "训练次数：4400，Loss：1.1239440441131592\n",
      "训练次数：4500，Loss：1.3776509761810303\n",
      "训练次数：4600，Loss：1.444976568222046\n",
      "整体测试集上的Loss：240.92126429080963\n",
      "整体测试集上的正确率：0.444599986076355\n",
      "模型已保存\n",
      "-----第 7 轮训练开始-----\n",
      "训练次数：4700，Loss：1.3373829126358032\n",
      "训练次数：4800，Loss：1.527467966079712\n",
      "训练次数：4900，Loss：1.3941456079483032\n",
      "训练次数：5000，Loss：1.347693920135498\n",
      "训练次数：5100，Loss：1.019992709159851\n",
      "训练次数：5200，Loss：1.3338412046432495\n",
      "训练次数：5300，Loss：1.2063201665878296\n",
      "训练次数：5400，Loss：1.376579999923706\n",
      "整体测试集上的Loss：230.11682534217834\n",
      "整体测试集上的正确率：0.47119998931884766\n",
      "模型已保存\n",
      "-----第 8 轮训练开始-----\n",
      "训练次数：5500，Loss：1.2322062253952026\n",
      "训练次数：5600，Loss：1.155503749847412\n",
      "训练次数：5700，Loss：1.2562335729599\n",
      "训练次数：5800，Loss：1.1995525360107422\n",
      "训练次数：5900，Loss：1.3068888187408447\n",
      "训练次数：6000，Loss：1.5647969245910645\n",
      "训练次数：6100，Loss：1.0451785326004028\n",
      "训练次数：6200，Loss：1.0634310245513916\n",
      "整体测试集上的Loss：218.9085488319397\n",
      "整体测试集上的正确率：0.5024999976158142\n",
      "模型已保存\n",
      "-----第 9 轮训练开始-----\n",
      "训练次数：6300，Loss：1.4393006563186646\n",
      "训练次数：6400，Loss：1.102798342704773\n",
      "训练次数：6500，Loss：1.5598957538604736\n",
      "训练次数：6600，Loss：1.1095472574234009\n",
      "训练次数：6700，Loss：1.0848497152328491\n",
      "训练次数：6800，Loss：1.1287778615951538\n",
      "训练次数：6900，Loss：1.0856565237045288\n",
      "训练次数：7000，Loss：0.9008570909500122\n",
      "整体测试集上的Loss：206.67768096923828\n",
      "整体测试集上的正确率：0.5324000120162964\n",
      "模型已保存\n",
      "-----第 10 轮训练开始-----\n",
      "训练次数：7100，Loss：1.2701932191848755\n",
      "训练次数：7200，Loss：0.9645299911499023\n",
      "训练次数：7300，Loss：1.1552343368530273\n",
      "训练次数：7400，Loss：0.8400740027427673\n",
      "训练次数：7500，Loss：1.1549839973449707\n",
      "训练次数：7600，Loss：1.2434695959091187\n",
      "训练次数：7700，Loss：0.874816358089447\n",
      "训练次数：7800，Loss：1.2162028551101685\n",
      "整体测试集上的Loss：196.87594783306122\n",
      "整体测试集上的正确率：0.5539000034332275\n",
      "模型已保存\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. GPU训练时间"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T13:03:14.178317Z",
     "start_time": "2025-02-17T13:02:35.335530Z"
    }
   },
   "source": [
    "import torchvision\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "\n",
    "# from model import * 相当于把 model中的所有内容写到这里，这里直接把 model 写在这里\n",
    "class Tudui(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Tudui, self).__init__()        \n",
    "        self.model1 = nn.Sequential(\n",
    "            nn.Conv2d(3,32,5,1,2),  # 输入通道3，输出通道32，卷积核尺寸5×5，步长1，填充2    \n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32,32,5,1,2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32,64,5,1,2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),  # 展平后变成 64*4*4 了\n",
    "            nn.Linear(64*4*4,64),\n",
    "            nn.Linear(64,10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model1(x)\n",
    "        return x\n",
    "\n",
    "# 准备数据集\n",
    "train_data = torchvision.datasets.CIFAR10(\"./data\",train=True,transform=torchvision.transforms.ToTensor(),download=True)\n",
    "test_data = torchvision.datasets.CIFAR10(\"./data\",train=False,transform=torchvision.transforms.ToTensor(),download=True)\n",
    "\n",
    "# length 长度\n",
    "train_data_size = len(train_data)\n",
    "test_data_size = len(test_data)\n",
    "# 如果train_data_size=10，则打印：训练数据集的长度为：10\n",
    "print(\"训练数据集的长度：{}\".format(train_data_size))\n",
    "print(\"测试数据集的长度：{}\".format(test_data_size))\n",
    "\n",
    "# 利用 Dataloader 来加载数据集\n",
    "train_dataloader = DataLoader(train_data, batch_size=64)        \n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "# 创建网络模型\n",
    "tudui = Tudui() \n",
    "if torch.cuda.is_available():\n",
    "    tudui = tudui.cuda() # 网络模型转移到cuda上\n",
    "\n",
    "# 损失函数\n",
    "loss_fn = nn.CrossEntropyLoss() # 交叉熵，fn 是 fuction 的缩写\n",
    "if torch.cuda.is_available():\n",
    "    loss_fn = loss_fn.cuda()        # 损失函数转移到cuda上\n",
    "\n",
    "# 优化器\n",
    "learning = 0.01  # 1e-2 就是 0.01 的意思\n",
    "optimizer = torch.optim.SGD(tudui.parameters(),learning)   # 随机梯度下降优化器  \n",
    "\n",
    "# 设置网络的一些参数\n",
    "# 记录训练的次数\n",
    "total_train_step = 0\n",
    "# 记录测试的次数\n",
    "total_test_step = 0\n",
    "\n",
    "# 训练的轮次\n",
    "epoch = 10\n",
    "\n",
    "# 添加 tensorboard\n",
    "writer = SummaryWriter(\"logs\")\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(epoch):\n",
    "    print(\"-----第 {} 轮训练开始-----\".format(i+1))\n",
    "    \n",
    "    # 训练步骤开始\n",
    "    tudui.train() # 当网络中有dropout层、batchnorm层时，这些层能起作用\n",
    "    for data in train_dataloader:\n",
    "        imgs, targets = data\n",
    "        if torch.cuda.is_available():\n",
    "            imgs = imgs.cuda()  # 数据放到cuda上\n",
    "            targets = targets.cuda() # 数据放到cuda上\n",
    "        outputs = tudui(imgs)\n",
    "        loss = loss_fn(outputs, targets) # 计算实际输出与目标输出的差距\n",
    "        \n",
    "        # 优化器对模型调优\n",
    "        optimizer.zero_grad()  # 梯度清零\n",
    "        loss.backward() # 反向传播，计算损失函数的梯度\n",
    "        optimizer.step()   # 根据梯度，对网络的参数进行调优\n",
    "        \n",
    "        total_train_step = total_train_step + 1\n",
    "        if total_train_step % 100 == 0:\n",
    "            end_time = time.time()\n",
    "            print(end_time - start_time) # 运行训练一百次后的时间间隔\n",
    "            print(\"训练次数：{}，Loss：{}\".format(total_train_step,loss.item()))  # 方式二：获得loss值\n",
    "            writer.add_scalar(\"train_loss\",loss.item(),total_train_step)\n",
    "    \n",
    "    # 测试步骤开始（每一轮训练后都查看在测试数据集上的loss情况）\n",
    "    tudui.eval()  # 当网络中有dropout层、batchnorm层时，这些层不能起作用\n",
    "    total_test_loss = 0\n",
    "    total_accuracy = 0\n",
    "    with torch.no_grad():  # 没有梯度了\n",
    "        for data in test_dataloader: # 测试数据集提取数据\n",
    "            imgs, targets = data # 数据放到cuda上\n",
    "            if torch.cuda.is_available():\n",
    "                imgs = imgs.cuda() # 数据放到cuda上\n",
    "                targets = targets.cuda()\n",
    "            outputs = tudui(imgs)\n",
    "            loss = loss_fn(outputs, targets) # 仅data数据在网络模型上的损失\n",
    "            total_test_loss = total_test_loss + loss.item() # 所有loss\n",
    "            accuracy = (outputs.argmax(1) == targets).sum()\n",
    "            total_accuracy = total_accuracy + accuracy\n",
    "            \n",
    "    print(\"整体测试集上的Loss：{}\".format(total_test_loss))\n",
    "    print(\"整体测试集上的正确率：{}\".format(total_accuracy/test_data_size))\n",
    "    writer.add_scalar(\"test_loss\",total_test_loss,total_test_step)\n",
    "    writer.add_scalar(\"test_accuracy\",total_accuracy/test_data_size,total_test_step)  \n",
    "    total_test_step = total_test_step + 1\n",
    "    \n",
    "    torch.save(tudui, \"./model/tudui_{}.pth\".format(i)) # 保存每一轮训练后的结果\n",
    "    #torch.save(tudui.state_dict(),\"tudui_{}.path\".format(i)) # 保存方式二         \n",
    "    print(\"模型已保存\")\n",
    "    \n",
    "writer.close()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据集的长度：50000\n",
      "测试数据集的长度：10000\n",
      "-----第 1 轮训练开始-----\n",
      "0.5046722888946533\n",
      "训练次数：100，Loss：2.2866806983947754\n",
      "0.9408602714538574\n",
      "训练次数：200，Loss：2.2828242778778076\n",
      "1.3448266983032227\n",
      "训练次数：300，Loss：2.2647433280944824\n",
      "1.7376956939697266\n",
      "训练次数：400，Loss：2.1721160411834717\n",
      "2.103969097137451\n",
      "训练次数：500，Loss：2.041794538497925\n",
      "2.47170090675354\n",
      "训练次数：600，Loss：2.023949384689331\n",
      "2.8493332862854004\n",
      "训练次数：700，Loss：2.0084164142608643\n",
      "整体测试集上的Loss：313.9958437681198\n",
      "整体测试集上的正确率：0.2874000072479248\n",
      "模型已保存\n",
      "-----第 2 轮训练开始-----\n",
      "3.685159921646118\n",
      "训练次数：800，Loss：1.8605778217315674\n",
      "4.055533170700073\n",
      "训练次数：900，Loss：1.852036476135254\n",
      "4.44176721572876\n",
      "训练次数：1000，Loss：1.9133800268173218\n",
      "4.831825017929077\n",
      "训练次数：1100，Loss：1.9484550952911377\n",
      "5.24647068977356\n",
      "训练次数：1200，Loss：1.7004770040512085\n",
      "5.660477876663208\n",
      "训练次数：1300，Loss：1.662767767906189\n",
      "6.0797343254089355\n",
      "训练次数：1400，Loss：1.731192708015442\n",
      "6.503046751022339\n",
      "训练次数：1500，Loss：1.8315141201019287\n",
      "整体测试集上的Loss：292.242916226387\n",
      "整体测试集上的正确率：0.3319999873638153\n",
      "模型已保存\n",
      "-----第 3 轮训练开始-----\n",
      "7.509714365005493\n",
      "训练次数：1600，Loss：1.767741322517395\n",
      "7.926432371139526\n",
      "训练次数：1700，Loss：1.6467843055725098\n",
      "8.350774049758911\n",
      "训练次数：1800，Loss：1.953343391418457\n",
      "8.774729013442993\n",
      "训练次数：1900，Loss：1.7135111093521118\n",
      "9.201693058013916\n",
      "训练次数：2000，Loss：1.896488070487976\n",
      "9.635545253753662\n",
      "训练次数：2100，Loss：1.5093659162521362\n",
      "10.060796737670898\n",
      "训练次数：2200，Loss：1.4745515584945679\n",
      "10.490480899810791\n",
      "训练次数：2300，Loss：1.7906625270843506\n",
      "整体测试集上的Loss：264.3360882997513\n",
      "整体测试集上的正确率：0.39229997992515564\n",
      "模型已保存\n",
      "-----第 4 轮训练开始-----\n",
      "11.555234670639038\n",
      "训练次数：2400，Loss：1.747369647026062\n",
      "12.053706884384155\n",
      "训练次数：2500，Loss：1.3792448043823242\n",
      "12.441597938537598\n",
      "训练次数：2600，Loss：1.564719557762146\n",
      "12.830574035644531\n",
      "训练次数：2700，Loss：1.6832406520843506\n",
      "13.198557615280151\n",
      "训练次数：2800，Loss：1.4551061391830444\n",
      "13.562926054000854\n",
      "训练次数：2900，Loss：1.6179420948028564\n",
      "13.923176527023315\n",
      "训练次数：3000，Loss：1.3520939350128174\n",
      "14.281939029693604\n",
      "训练次数：3100，Loss：1.5074485540390015\n",
      "整体测试集上的Loss：262.898735165596\n",
      "整体测试集上的正确率：0.396699994802475\n",
      "模型已保存\n",
      "-----第 5 轮训练开始-----\n",
      "15.095556020736694\n",
      "训练次数：3200，Loss：1.358004093170166\n",
      "15.471822023391724\n",
      "训练次数：3300，Loss：1.4669296741485596\n",
      "15.855393648147583\n",
      "训练次数：3400，Loss：1.4900670051574707\n",
      "16.318858861923218\n",
      "训练次数：3500，Loss：1.5727993249893188\n",
      "16.72993540763855\n",
      "训练次数：3600，Loss：1.5713598728179932\n",
      "17.18407702445984\n",
      "训练次数：3700，Loss：1.3122048377990723\n",
      "17.65215539932251\n",
      "训练次数：3800，Loss：1.2720046043395996\n",
      "18.0712308883667\n",
      "训练次数：3900，Loss：1.395328402519226\n",
      "整体测试集上的Loss：252.00753092765808\n",
      "整体测试集上的正确率：0.42069998383522034\n",
      "模型已保存\n",
      "-----第 6 轮训练开始-----\n",
      "19.145978450775146\n",
      "训练次数：4000，Loss：1.3849308490753174\n",
      "19.517306327819824\n",
      "训练次数：4100，Loss：1.410287618637085\n",
      "19.88571286201477\n",
      "训练次数：4200，Loss：1.5347375869750977\n",
      "20.2853524684906\n",
      "训练次数：4300，Loss：1.2073674201965332\n",
      "20.678675174713135\n",
      "训练次数：4400，Loss：1.1566476821899414\n",
      "21.058335542678833\n",
      "训练次数：4500，Loss：1.3671393394470215\n",
      "21.43400263786316\n",
      "训练次数：4600，Loss：1.401909351348877\n",
      "整体测试集上的Loss：238.53390848636627\n",
      "整体测试集上的正确率：0.4490000009536743\n",
      "模型已保存\n",
      "-----第 7 轮训练开始-----\n",
      "22.311871767044067\n",
      "训练次数：4700，Loss：1.3704296350479126\n",
      "22.703421354293823\n",
      "训练次数：4800，Loss：1.4997215270996094\n",
      "23.098761558532715\n",
      "训练次数：4900，Loss：1.4309834241867065\n",
      "23.504927396774292\n",
      "训练次数：5000，Loss：1.4148125648498535\n",
      "23.924688577651978\n",
      "训练次数：5100，Loss：1.0342292785644531\n",
      "24.351779460906982\n",
      "训练次数：5200，Loss：1.3707889318466187\n",
      "24.773683547973633\n",
      "训练次数：5300，Loss：1.1814754009246826\n",
      "25.19127869606018\n",
      "训练次数：5400，Loss：1.42862868309021\n",
      "整体测试集上的Loss：227.86727035045624\n",
      "整体测试集上的正确率：0.4717999994754791\n",
      "模型已保存\n",
      "-----第 8 轮训练开始-----\n",
      "26.185722827911377\n",
      "训练次数：5500，Loss：1.2078479528427124\n",
      "26.607879161834717\n",
      "训练次数：5600，Loss：1.2011338472366333\n",
      "27.039581060409546\n",
      "训练次数：5700，Loss：1.2036149501800537\n",
      "27.54207158088684\n",
      "训练次数：5800，Loss：1.2203116416931152\n",
      "27.964544773101807\n",
      "训练次数：5900，Loss：1.3551527261734009\n",
      "28.44314169883728\n",
      "训练次数：6000，Loss：1.5354827642440796\n",
      "28.872987031936646\n",
      "训练次数：6100，Loss：1.0403426885604858\n",
      "29.249764680862427\n",
      "训练次数：6200，Loss：1.131527066230774\n",
      "整体测试集上的Loss：217.81267499923706\n",
      "整体测试集上的正确率：0.49879997968673706\n",
      "模型已保存\n",
      "-----第 9 轮训练开始-----\n",
      "30.086116552352905\n",
      "训练次数：6300，Loss：1.3224925994873047\n",
      "30.4821879863739\n",
      "训练次数：6400，Loss：1.1232552528381348\n",
      "30.885862588882446\n",
      "训练次数：6500，Loss：1.501425862312317\n",
      "31.317529916763306\n",
      "训练次数：6600，Loss：1.1334813833236694\n",
      "31.73517394065857\n",
      "训练次数：6700，Loss：1.0856516361236572\n",
      "32.15814781188965\n",
      "训练次数：6800，Loss：1.1489677429199219\n",
      "32.61242604255676\n",
      "训练次数：6900，Loss：1.121505856513977\n",
      "33.04233193397522\n",
      "训练次数：7000，Loss：0.9943914413452148\n",
      "整体测试集上的Loss：209.64855670928955\n",
      "整体测试集上的正确率：0.5220999717712402\n",
      "模型已保存\n",
      "-----第 10 轮训练开始-----\n",
      "34.06907939910889\n",
      "训练次数：7100，Loss：1.2652262449264526\n",
      "34.50459003448486\n",
      "训练次数：7200，Loss：0.9950842261314392\n",
      "34.924647092819214\n",
      "训练次数：7300，Loss：1.102644681930542\n",
      "35.34461164474487\n",
      "训练次数：7400，Loss：0.8494997024536133\n",
      "35.7777304649353\n",
      "训练次数：7500，Loss：1.1883950233459473\n",
      "36.213403940200806\n",
      "训练次数：7600，Loss：1.2128230333328247\n",
      "36.63671016693115\n",
      "训练次数：7700，Loss：0.8959314823150635\n",
      "37.058871030807495\n",
      "训练次数：7800，Loss：1.220979928970337\n",
      "整体测试集上的Loss：201.81553900241852\n",
      "整体测试集上的正确率：0.545199990272522\n",
      "模型已保存\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. CPU训练时间"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T13:04:03.559495Z",
     "start_time": "2025-02-17T13:03:52.994532Z"
    }
   },
   "source": [
    "import torchvision\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "\n",
    "# from model import * 相当于把 model中的所有内容写到这里，这里直接把 model 写在这里\n",
    "class Tudui(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Tudui, self).__init__()        \n",
    "        self.model1 = nn.Sequential(\n",
    "            nn.Conv2d(3,32,5,1,2),  # 输入通道3，输出通道32，卷积核尺寸5×5，步长1，填充2    \n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32,32,5,1,2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32,64,5,1,2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),  # 展平后变成 64*4*4 了\n",
    "            nn.Linear(64*4*4,64),\n",
    "            nn.Linear(64,10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model1(x)\n",
    "        return x\n",
    "\n",
    "# 准备数据集\n",
    "train_data = torchvision.datasets.CIFAR10(\"./data\",train=True,transform=torchvision.transforms.ToTensor(),download=True)\n",
    "test_data = torchvision.datasets.CIFAR10(\"./data\",train=False,transform=torchvision.transforms.ToTensor(),download=True)\n",
    "\n",
    "# length 长度\n",
    "train_data_size = len(train_data)\n",
    "test_data_size = len(test_data)\n",
    "# 如果train_data_size=10，则打印：训练数据集的长度为：10\n",
    "print(\"训练数据集的长度：{}\".format(train_data_size))\n",
    "print(\"测试数据集的长度：{}\".format(test_data_size))\n",
    "\n",
    "# 利用 Dataloader 来加载数据集\n",
    "train_dataloader = DataLoader(train_data, batch_size=64)        \n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "# 创建网络模型\n",
    "tudui = Tudui() \n",
    "\n",
    "# 损失函数\n",
    "loss_fn = nn.CrossEntropyLoss() # 交叉熵，fn 是 fuction 的缩写\n",
    "\n",
    "# 优化器\n",
    "learning = 0.01  # 1e-2 就是 0.01 的意思\n",
    "optimizer = torch.optim.SGD(tudui.parameters(),learning)   # 随机梯度下降优化器  \n",
    "\n",
    "# 设置网络的一些参数\n",
    "# 记录训练的次数\n",
    "total_train_step = 0\n",
    "# 记录测试的次数\n",
    "total_test_step = 0\n",
    "\n",
    "# 训练的轮次\n",
    "epoch = 10\n",
    "\n",
    "# 添加 tensorboard\n",
    "writer = SummaryWriter(\"logs\")\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(epoch):\n",
    "    print(\"-----第 {} 轮训练开始-----\".format(i+1))\n",
    "    \n",
    "    # 训练步骤开始\n",
    "    tudui.train() # 当网络中有dropout层、batchnorm层时，这些层能起作用\n",
    "    for data in train_dataloader:\n",
    "        imgs, targets = data\n",
    "        outputs = tudui(imgs)\n",
    "        loss = loss_fn(outputs, targets) # 计算实际输出与目标输出的差距\n",
    "        \n",
    "        # 优化器对模型调优\n",
    "        optimizer.zero_grad()  # 梯度清零\n",
    "        loss.backward() # 反向传播，计算损失函数的梯度\n",
    "        optimizer.step()   # 根据梯度，对网络的参数进行调优\n",
    "        \n",
    "        total_train_step = total_train_step + 1\n",
    "        if total_train_step % 100 == 0:\n",
    "            end_time = time.time()\n",
    "            print(end_time - start_time) # 运行训练一百次后的时间间隔\n",
    "            print(\"训练次数：{}，Loss：{}\".format(total_train_step,loss.item()))  # 方式二：获得loss值\n",
    "            writer.add_scalar(\"train_loss\",loss.item(),total_train_step)\n",
    "    \n",
    "    # 测试步骤开始（每一轮训练后都查看在测试数据集上的loss情况）\n",
    "    tudui.eval()  # 当网络中有dropout层、batchnorm层时，这些层不能起作用\n",
    "    total_test_loss = 0\n",
    "    total_accuracy = 0\n",
    "    with torch.no_grad():  # 没有梯度了\n",
    "        for data in test_dataloader: # 测试数据集提取数据\n",
    "            imgs, targets = data \n",
    "            outputs = tudui(imgs)\n",
    "            loss = loss_fn(outputs, targets) # 仅data数据在网络模型上的损失\n",
    "            total_test_loss = total_test_loss + loss.item() # 所有loss\n",
    "            accuracy = (outputs.argmax(1) == targets).sum()\n",
    "            total_accuracy = total_accuracy + accuracy\n",
    "            \n",
    "    print(\"整体测试集上的Loss：{}\".format(total_test_loss))\n",
    "    print(\"整体测试集上的正确率：{}\".format(total_accuracy/test_data_size))\n",
    "    writer.add_scalar(\"test_loss\",total_test_loss,total_test_step)\n",
    "    writer.add_scalar(\"test_accuracy\",total_accuracy/test_data_size,total_test_step)  \n",
    "    total_test_step = total_test_step + 1\n",
    "    \n",
    "    torch.save(tudui, \"./model/tudui_{}.pth\".format(i)) # 保存每一轮训练后的结果\n",
    "    #torch.save(tudui.state_dict(),\"tudui_{}.path\".format(i)) # 保存方式二         \n",
    "    print(\"模型已保存\")\n",
    "    \n",
    "writer.close()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据集的长度：50000\n",
      "测试数据集的长度：10000\n",
      "-----第 1 轮训练开始-----\n",
      "1.4352877140045166\n",
      "训练次数：100，Loss：2.2918057441711426\n",
      "2.675241470336914\n",
      "训练次数：200，Loss：2.292357921600342\n",
      "3.9018073081970215\n",
      "训练次数：300，Loss：2.2770802974700928\n",
      "5.126612663269043\n",
      "训练次数：400，Loss：2.2110376358032227\n",
      "6.338543176651001\n",
      "训练次数：500，Loss：2.203845739364624\n",
      "7.574108839035034\n",
      "训练次数：600，Loss：2.1480653285980225\n",
      "8.807763814926147\n",
      "训练次数：700，Loss：2.0787978172302246\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 73\u001B[0m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m train_dataloader:\n\u001B[0;32m     72\u001B[0m     imgs, targets \u001B[38;5;241m=\u001B[39m data\n\u001B[1;32m---> 73\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m tudui(imgs)\n\u001B[0;32m     74\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss_fn(outputs, targets) \u001B[38;5;66;03m# 计算实际输出与目标输出的差距\u001B[39;00m\n\u001B[0;32m     76\u001B[0m     \u001B[38;5;66;03m# 优化器对模型调优\u001B[39;00m\n",
      "File \u001B[1;32mD:\\app\\conda_data\\envs\\my_dl\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\app\\conda_data\\envs\\my_dl\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[1;32mIn[5], line 25\u001B[0m, in \u001B[0;36mTudui.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m---> 25\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel1(x)\n\u001B[0;32m     26\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[1;32mD:\\app\\conda_data\\envs\\my_dl\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\app\\conda_data\\envs\\my_dl\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mD:\\app\\conda_data\\envs\\my_dl\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    248\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    249\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 250\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m module(\u001B[38;5;28minput\u001B[39m)\n\u001B[0;32m    251\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32mD:\\app\\conda_data\\envs\\my_dl\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\app\\conda_data\\envs\\my_dl\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mD:\\app\\conda_data\\envs\\my_dl\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    553\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 554\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_conv_forward(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias)\n",
      "File \u001B[1;32mD:\\app\\conda_data\\envs\\my_dl\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    537\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    538\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(\n\u001B[0;32m    539\u001B[0m         F\u001B[38;5;241m.\u001B[39mpad(\n\u001B[0;32m    540\u001B[0m             \u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    547\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups,\n\u001B[0;32m    548\u001B[0m     )\n\u001B[1;32m--> 549\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(\n\u001B[0;32m    550\u001B[0m     \u001B[38;5;28minput\u001B[39m, weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups\n\u001B[0;32m    551\u001B[0m )\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 利用GPU训练(方式二)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "① 电脑上有两个显卡时，可以用指定cuda:0、cuda:1。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T02:26:29.495596Z",
     "start_time": "2025-02-18T02:26:11.001696Z"
    }
   },
   "source": [
    "import torchvision\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "\n",
    "# 定义训练的设备\n",
    "#device = torch.device(\"cpu\")\n",
    "#device = torch.device(\"cuda\")   # 使用 GPU 方式一 \n",
    "#device = torch.device(\"cuda:0\") # 使用 GPU 方式二\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# from model import * 相当于把 model中的所有内容写到这里，这里直接把 model 写在这里\n",
    "class Tudui(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Tudui, self).__init__()        \n",
    "        self.model1 = nn.Sequential(\n",
    "            nn.Conv2d(3,32,5,1,2),  # 输入通道3，输出通道32，卷积核尺寸5×5，步长1，填充2    \n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32,32,5,1,2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32,64,5,1,2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),  # 展平后变成 64*4*4 了\n",
    "            nn.Linear(64*4*4,64),\n",
    "            nn.Linear(64,10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model1(x)\n",
    "        return x\n",
    "\n",
    "# 准备数据集\n",
    "train_data = torchvision.datasets.CIFAR10(\"./data\",train=True,transform=torchvision.transforms.ToTensor(),download=True)\n",
    "test_data = torchvision.datasets.CIFAR10(\"./data\",train=False,transform=torchvision.transforms.ToTensor(),download=True)\n",
    "\n",
    "# length 长度\n",
    "train_data_size = len(train_data)\n",
    "test_data_size = len(test_data)\n",
    "# 如果train_data_size=10，则打印：训练数据集的长度为：10\n",
    "print(\"训练数据集的长度：{}\".format(train_data_size))\n",
    "print(\"测试数据集的长度：{}\".format(test_data_size))\n",
    "\n",
    "# 利用 Dataloader 来加载数据集\n",
    "train_dataloader = DataLoader(train_data, batch_size=64)        \n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "# 创建网络模型\n",
    "tudui = Tudui() \n",
    "tudui = tudui.to(device) # 也可以不赋值，直接 tudui.to(device) \n",
    "\n",
    "\n",
    "# 损失函数\n",
    "loss_fn = nn.CrossEntropyLoss() # 交叉熵，fn 是 fuction 的缩写\n",
    "loss_fn = loss_fn.to(device) # 也可以不赋值，直接loss_fn.to(device)\n",
    "\n",
    "# 优化器\n",
    "learning = 0.01  # 1e-2 就是 0.01 的意思\n",
    "optimizer = torch.optim.SGD(tudui.parameters(),learning)   # 随机梯度下降优化器  \n",
    "\n",
    "# 设置网络的一些参数\n",
    "# 记录训练的次数\n",
    "total_train_step = 0\n",
    "# 记录测试的次数\n",
    "total_test_step = 0\n",
    "\n",
    "# 训练的轮次\n",
    "epoch = 10\n",
    "\n",
    "# 添加 tensorboard\n",
    "writer = SummaryWriter(\"logs\")\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(epoch):\n",
    "    print(\"-----第 {} 轮训练开始-----\".format(i+1))\n",
    "    \n",
    "    # 训练步骤开始\n",
    "    tudui.train() # 当网络中有dropout层、batchnorm层时，这些层能起作用\n",
    "    for data in train_dataloader:\n",
    "        imgs, targets = data\n",
    "        imgs = imgs.to(device) # 也可以不赋值，直接 imgs.to(device)\n",
    "        targets = targets.to(device) # 也可以不赋值，直接 targets.to(device)\n",
    "        outputs = tudui(imgs)\n",
    "        loss = loss_fn(outputs, targets) # 计算实际输出与目标输出的差距\n",
    "        \n",
    "        # 优化器对模型调优\n",
    "        optimizer.zero_grad()  # 梯度清零\n",
    "        loss.backward() # 反向传播，计算损失函数的梯度\n",
    "        optimizer.step()   # 根据梯度，对网络的参数进行调优\n",
    "        \n",
    "        total_train_step = total_train_step + 1\n",
    "        if total_train_step % 100 == 0:\n",
    "            end_time = time.time()\n",
    "            print(end_time - start_time) # 运行训练一百次后的时间间隔\n",
    "            print(\"训练次数：{}，Loss：{}\".format(total_train_step,loss.item()))  # 方式二：获得loss值\n",
    "            writer.add_scalar(\"train_loss\",loss.item(),total_train_step)\n",
    "    \n",
    "    # 测试步骤开始（每一轮训练后都查看在测试数据集上的loss情况）\n",
    "    tudui.eval()  # 当网络中有dropout层、batchnorm层时，这些层不能起作用\n",
    "    total_test_loss = 0\n",
    "    total_accuracy = 0\n",
    "    with torch.no_grad():  # 没有梯度了\n",
    "        for data in test_dataloader: # 测试数据集提取数据\n",
    "            imgs, targets = data # 数据放到cuda上\n",
    "            imgs = imgs.to(device) # 也可以不赋值，直接 imgs.to(device)\n",
    "            targets = targets.to(device) # 也可以不赋值，直接 targets.to(device)\n",
    "            outputs = tudui(imgs)\n",
    "            loss = loss_fn(outputs, targets) # 仅data数据在网络模型上的损失\n",
    "            total_test_loss = total_test_loss + loss.item() # 所有loss\n",
    "            accuracy = (outputs.argmax(1) == targets).sum()\n",
    "            total_accuracy = total_accuracy + accuracy\n",
    "            \n",
    "    print(\"整体测试集上的Loss：{}\".format(total_test_loss))\n",
    "    print(\"整体测试集上的正确率：{}\".format(total_accuracy/test_data_size))\n",
    "    writer.add_scalar(\"test_loss\",total_test_loss,total_test_step)\n",
    "    writer.add_scalar(\"test_accuracy\",total_accuracy/test_data_size,total_test_step)  \n",
    "    total_test_step = total_test_step + 1\n",
    "    \n",
    "    torch.save(tudui, \"./model/tudui_{}.pth\".format(i)) # 保存每一轮训练后的结果\n",
    "    #torch.save(tudui.state_dict(),\"tudui_{}.path\".format(i)) # 保存方式二         \n",
    "    print(\"模型已保存\")\n",
    "    \n",
    "writer.close()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据集的长度：50000\n",
      "测试数据集的长度：10000\n",
      "-----第 1 轮训练开始-----\n",
      "0.6385691165924072\n",
      "训练次数：100，Loss：2.286257743835449\n",
      "1.051924228668213\n",
      "训练次数：200，Loss：2.289566993713379\n",
      "1.4706008434295654\n",
      "训练次数：300，Loss：2.2719533443450928\n",
      "1.8986375331878662\n",
      "训练次数：400，Loss：2.219691514968872\n",
      "2.3060693740844727\n",
      "训练次数：500，Loss：2.1841814517974854\n",
      "2.720306873321533\n",
      "训练次数：600，Loss：2.073662519454956\n",
      "3.1332085132598877\n",
      "训练次数：700，Loss：2.0191469192504883\n",
      "整体测试集上的Loss：318.26700806617737\n",
      "整体测试集上的正确率：0.26969999074935913\n",
      "模型已保存\n",
      "-----第 2 轮训练开始-----\n",
      "4.1012208461761475\n",
      "训练次数：800，Loss：1.9125475883483887\n",
      "4.522088289260864\n",
      "训练次数：900，Loss：1.898923397064209\n",
      "4.933859348297119\n",
      "训练次数：1000，Loss：1.9539339542388916\n",
      "5.357146978378296\n",
      "训练次数：1100，Loss：1.9980418682098389\n",
      "5.757080078125\n",
      "训练次数：1200，Loss：1.7408089637756348\n",
      "6.1695966720581055\n",
      "训练次数：1300，Loss：1.7018464803695679\n",
      "6.584136724472046\n",
      "训练次数：1400，Loss：1.7694170475006104\n",
      "6.994166374206543\n",
      "训练次数：1500，Loss：1.8063325881958008\n",
      "整体测试集上的Loss：302.3266851902008\n",
      "整体测试集上的正确率：0.30469998717308044\n",
      "模型已保存\n",
      "-----第 3 轮训练开始-----\n",
      "7.919118881225586\n",
      "训练次数：1600，Loss：1.7918715476989746\n",
      "8.348926782608032\n",
      "训练次数：1700，Loss：1.6874500513076782\n",
      "8.79786205291748\n",
      "训练次数：1800，Loss：1.933786392211914\n",
      "9.266068696975708\n",
      "训练次数：1900，Loss：1.711432933807373\n",
      "9.743385314941406\n",
      "训练次数：2000，Loss：1.9225642681121826\n",
      "10.211842775344849\n",
      "训练次数：2100，Loss：1.535691499710083\n",
      "10.680848360061646\n",
      "训练次数：2200，Loss：1.4747536182403564\n",
      "11.16974949836731\n",
      "训练次数：2300，Loss：1.7750362157821655\n",
      "整体测试集上的Loss：266.49482345581055\n",
      "整体测试集上的正确率：0.37790000438690186\n",
      "模型已保存\n",
      "-----第 4 轮训练开始-----\n",
      "12.305827379226685\n",
      "训练次数：2400，Loss：1.732819676399231\n",
      "12.794739484786987\n",
      "训练次数：2500，Loss：1.3853574991226196\n",
      "13.268697500228882\n",
      "训练次数：2600，Loss：1.6127705574035645\n",
      "13.755741834640503\n",
      "训练次数：2700，Loss：1.6687105894088745\n",
      "14.245527982711792\n",
      "训练次数：2800，Loss：1.480881690979004\n",
      "14.72939395904541\n",
      "训练次数：2900，Loss：1.585692286491394\n",
      "15.195183992385864\n",
      "训练次数：3000，Loss：1.3648922443389893\n",
      "15.705406427383423\n",
      "训练次数：3100，Loss：1.5486797094345093\n",
      "整体测试集上的Loss：263.26596319675446\n",
      "整体测试集上的正确率：0.38839998841285706\n",
      "模型已保存\n",
      "-----第 5 轮训练开始-----\n",
      "16.870319366455078\n",
      "训练次数：3200，Loss：1.3644218444824219\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 80\u001B[0m\n\u001B[0;32m     78\u001B[0m \u001B[38;5;66;03m# 训练步骤开始\u001B[39;00m\n\u001B[0;32m     79\u001B[0m tudui\u001B[38;5;241m.\u001B[39mtrain() \u001B[38;5;66;03m# 当网络中有dropout层、batchnorm层时，这些层能起作用\u001B[39;00m\n\u001B[1;32m---> 80\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m train_dataloader:\n\u001B[0;32m     81\u001B[0m     imgs, targets \u001B[38;5;241m=\u001B[39m data            \n\u001B[0;32m     82\u001B[0m     imgs \u001B[38;5;241m=\u001B[39m imgs\u001B[38;5;241m.\u001B[39mto(device) \u001B[38;5;66;03m# 也可以不赋值，直接 imgs.to(device)\u001B[39;00m\n",
      "File \u001B[1;32mD:\\app\\conda_data\\envs\\my_dl\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:704\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    703\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__next__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m--> 704\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mrecord_function(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_profile_name):\n\u001B[0;32m    705\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    706\u001B[0m             \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    707\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n",
      "File \u001B[1;32mD:\\app\\conda_data\\envs\\my_dl\\Lib\\site-packages\\torch\\autograd\\profiler.py:757\u001B[0m, in \u001B[0;36mrecord_function.__exit__\u001B[1;34m(self, exc_type, exc_value, traceback)\u001B[0m\n\u001B[0;32m    752\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrecord \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39m_record_function_enter_new(\n\u001B[0;32m    753\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\n\u001B[0;32m    754\u001B[0m     )\n\u001B[0;32m    755\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n\u001B[1;32m--> 757\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__exit__\u001B[39m(\u001B[38;5;28mself\u001B[39m, exc_type: Any, exc_value: Any, traceback: Any):\n\u001B[0;32m    758\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrun_callbacks_on_exit:\n\u001B[0;32m    759\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 运行Terminal语句"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "① 运行terminal上运行的命令，可以在代码块中输入语句，在语句前加一个感叹号。\n",
    "\n",
    "② 输入 !nvidia-smi，可以查看显卡配置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 31 17:24:49 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 471.35       Driver Version: 471.35       CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   61C    P0    47W /  N/A |   2913MiB / 16384MiB |     10%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1868    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A     14152    C+G   ...4__htrsf667h5kn2\\AWCC.exe    N/A      |\n",
      "|    0   N/A  N/A     14904    C+G   ...2\\extracted\\WeChatApp.exe    N/A      |\n",
      "|    0   N/A  N/A     19304    C+G   ...y\\AccountsControlHost.exe    N/A      |\n",
      "|    0   N/A  N/A     21816    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     23044    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A     23480    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     24180    C+G   ...tracted\\WechatBrowser.exe    N/A      |\n",
      "|    0   N/A  N/A     24376    C+G   ...erver\\YourPhoneServer.exe    N/A      |\n",
      "|    0   N/A  N/A     24912    C+G   ...kzcwy\\mcafee-security.exe    N/A      |\n",
      "|    0   N/A  N/A     25524    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     27768    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A     27788      C   ...a\\envs\\py3.6.3\\python.exe    N/A      |\n",
      "|    0   N/A  N/A     27960    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     31320    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A     32796    C+G   ...e\\StoreExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     35728    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "350px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
