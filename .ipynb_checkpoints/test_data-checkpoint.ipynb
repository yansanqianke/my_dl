{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-18T02:00:37.657063Z",
     "start_time": "2025-02-18T02:00:37.099013Z"
    }
   },
   "source": [
    "import torchvision\n",
    "\n",
    "train_data = torchvision.datasets.CIFAR10(\"./data\",train=True,transform=torchvision.transforms.ToTensor(),download=True)\n",
    "imgs, targets=train_data\n",
    "print(targets)"
   ],
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "duplicate registrations for aten.linspace.Tensor_Tensor",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m\n\u001B[0;32m      3\u001B[0m train_data \u001B[38;5;241m=\u001B[39m torchvision\u001B[38;5;241m.\u001B[39mdatasets\u001B[38;5;241m.\u001B[39mCIFAR10(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./data\u001B[39m\u001B[38;5;124m\"\u001B[39m,train\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,transform\u001B[38;5;241m=\u001B[39mtorchvision\u001B[38;5;241m.\u001B[39mtransforms\u001B[38;5;241m.\u001B[39mToTensor(),download\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m      4\u001B[0m imgs, targets\u001B[38;5;241m=\u001B[39mtrain_data\n",
      "File \u001B[1;32mD:\\app\\conda_data\\envs\\my_dl\\Lib\\site-packages\\torchvision\\__init__.py:5\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwarnings\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmodulefinder\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Module\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# .extensions) before entering _meta_registrations.\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mextension\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _HAS_OPS  \u001B[38;5;66;03m# usort:skip\u001B[39;00m\n",
      "File \u001B[1;32mD:\\app\\conda_data\\envs\\my_dl\\Lib\\site-packages\\torch\\__init__.py:2604\u001B[0m\n\u001B[0;32m   2600\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m vmap \u001B[38;5;28;01mas\u001B[39;00m vmap\n\u001B[0;32m   2603\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m TYPE_CHECKING:\n\u001B[1;32m-> 2604\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _meta_registrations\n\u001B[0;32m   2606\u001B[0m \u001B[38;5;66;03m# Enable CUDA Sanitizer\u001B[39;00m\n\u001B[0;32m   2607\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTORCH_CUDA_SANITIZER\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m os\u001B[38;5;241m.\u001B[39menviron:\n",
      "File \u001B[1;32mD:\\app\\conda_data\\envs\\my_dl\\Lib\\site-packages\\torch\\_meta_registrations.py:97\u001B[0m\n\u001B[0;32m     90\u001B[0m     broadcasted_shape \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m(_broadcast_shapes(self_shape, \u001B[38;5;241m*\u001B[39margs_shape))\n\u001B[0;32m     91\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_check(\n\u001B[0;32m     92\u001B[0m         broadcasted_shape \u001B[38;5;241m==\u001B[39m self_shape,\n\u001B[0;32m     93\u001B[0m         \u001B[38;5;28;01mlambda\u001B[39;00m: \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput with shape \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mself_shape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m doesn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt match the broadcast shape \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbroadcasted_shape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     94\u001B[0m     )\n\u001B[1;32m---> 97\u001B[0m \u001B[38;5;129m@register_meta\u001B[39m([aten\u001B[38;5;241m.\u001B[39mlinspace, aten\u001B[38;5;241m.\u001B[39mlogspace])\n\u001B[0;32m     98\u001B[0m \u001B[38;5;129m@out_wrapper\u001B[39m()\n\u001B[0;32m     99\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmeta_linspace_logspace\u001B[39m(\n\u001B[0;32m    100\u001B[0m     start,\n\u001B[0;32m    101\u001B[0m     end,\n\u001B[0;32m    102\u001B[0m     steps,\n\u001B[0;32m    103\u001B[0m     base\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    104\u001B[0m     dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    105\u001B[0m     device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    106\u001B[0m     layout\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mstrided,\n\u001B[0;32m    107\u001B[0m     pin_memory\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    108\u001B[0m     requires_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    109\u001B[0m ):\n\u001B[0;32m    110\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(start, torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[0;32m    111\u001B[0m         torch\u001B[38;5;241m.\u001B[39m_check(\n\u001B[0;32m    112\u001B[0m             start\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m,\n\u001B[0;32m    113\u001B[0m             \u001B[38;5;28;01mlambda\u001B[39;00m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlinspace only supports 0-dimensional start and end tensors\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    114\u001B[0m         )\n",
      "File \u001B[1;32mD:\\app\\conda_data\\envs\\my_dl\\Lib\\site-packages\\torch\\_meta_registrations.py:54\u001B[0m, in \u001B[0;36mregister_meta.<locals>.wrapper\u001B[1;34m(fn)\u001B[0m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mregister\u001B[39m(op):\n\u001B[0;32m     52\u001B[0m     _add_op_to_registry(meta_table, op, fn)\n\u001B[1;32m---> 54\u001B[0m pytree\u001B[38;5;241m.\u001B[39mtree_map_(register, op)\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m fn\n",
      "File \u001B[1;32mD:\\app\\conda_data\\envs\\my_dl\\Lib\\site-packages\\torch\\utils\\_pytree.py:1024\u001B[0m, in \u001B[0;36mtree_map_\u001B[1;34m(func, tree, is_leaf, *rests)\u001B[0m\n\u001B[0;32m   1022\u001B[0m leaves, treespec \u001B[38;5;241m=\u001B[39m tree_flatten(tree, is_leaf\u001B[38;5;241m=\u001B[39mis_leaf)\n\u001B[0;32m   1023\u001B[0m flat_args \u001B[38;5;241m=\u001B[39m [leaves] \u001B[38;5;241m+\u001B[39m [treespec\u001B[38;5;241m.\u001B[39mflatten_up_to(r) \u001B[38;5;28;01mfor\u001B[39;00m r \u001B[38;5;129;01min\u001B[39;00m rests]\n\u001B[1;32m-> 1024\u001B[0m deque(\u001B[38;5;28mmap\u001B[39m(func, \u001B[38;5;241m*\u001B[39mflat_args), maxlen\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)  \u001B[38;5;66;03m# consume and exhaust the iterable\u001B[39;00m\n\u001B[0;32m   1025\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m tree\n",
      "File \u001B[1;32mD:\\app\\conda_data\\envs\\my_dl\\Lib\\site-packages\\torch\\_meta_registrations.py:52\u001B[0m, in \u001B[0;36mregister_meta.<locals>.wrapper.<locals>.register\u001B[1;34m(op)\u001B[0m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mregister\u001B[39m(op):\n\u001B[1;32m---> 52\u001B[0m     _add_op_to_registry(meta_table, op, fn)\n",
      "File \u001B[1;32mD:\\app\\conda_data\\envs\\my_dl\\Lib\\site-packages\\torch\\_decomp\\__init__.py:95\u001B[0m, in \u001B[0;36m_add_op_to_registry\u001B[1;34m(registry, op, fn)\u001B[0m\n\u001B[0;32m     93\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m op_overload \u001B[38;5;129;01min\u001B[39;00m overloads:\n\u001B[0;32m     94\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m op_overload \u001B[38;5;129;01min\u001B[39;00m registry:\n\u001B[1;32m---> 95\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mduplicate registrations for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mop_overload\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     96\u001B[0m     \u001B[38;5;66;03m# TorchScript dumps a bunch of extra nonsense overloads\u001B[39;00m\n\u001B[0;32m     97\u001B[0m     \u001B[38;5;66;03m# which don't have corresponding dispatcher entries, we need\u001B[39;00m\n\u001B[0;32m     98\u001B[0m     \u001B[38;5;66;03m# to filter those out, e.g aten.add.float_int\u001B[39;00m\n\u001B[0;32m     99\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_dispatch_has_kernel(op_overload\u001B[38;5;241m.\u001B[39mname()):\n",
      "\u001B[1;31mRuntimeError\u001B[0m: duplicate registrations for aten.linspace.Tensor_Tensor"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
